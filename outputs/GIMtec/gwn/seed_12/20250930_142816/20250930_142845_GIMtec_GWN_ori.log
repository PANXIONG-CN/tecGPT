2025-09-30 14:28: Experiment log path in: outputs/GIMtec/gwn/seed_12/20250930_142816
2025-09-30 14:28: Command: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False
2025-09-30 14:28: Torch: 2.7.0+cu128  CUDA: True
2025-09-30 14:28: GPU: NVIDIA H800 PCIe
2025-09-30 14:28: CUDA_VISIBLE_DEVICES=None
2025-09-30 14:28: HS: 10
2025-09-30 14:28: HT: 16
2025-09-30 14:28: HT_Tem: 8
2025-09-30 14:28: _run_start_ts: 20250930_142816
2025-09-30 14:28: _train_start_time: 1759213725.5321095
2025-09-30 14:28: accumulate_steps: 1
2025-09-30 14:28: ada_mask_ratio: 1.0
2025-09-30 14:28: ada_type: all
2025-09-30 14:28: amp: True
2025-09-30 14:28: batch_size: 8
2025-09-30 14:28: change_epoch: 5
2025-09-30 14:28: cmdline: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False
2025-09-30 14:28: column_wise: False
2025-09-30 14:28: config: 
2025-09-30 14:28: cosine_t_max: 100
2025-09-30 14:28: cuda: True
2025-09-30 14:28: dataset: GIMtec
2025-09-30 14:28: debug: False
2025-09-30 14:28: debug_max_steps: 1000
2025-09-30 14:28: default_graph: True
2025-09-30 14:28: device: cuda
2025-09-30 14:28: early_stop: True
2025-09-30 14:28: early_stop_min_delta: 0.0
2025-09-30 14:28: early_stop_patience: 20
2025-09-30 14:28: embed_dim: 16
2025-09-30 14:28: embed_dim_spa: 4
2025-09-30 14:28: epochs: 1
2025-09-30 14:28: eta_min_factor: 0.01
2025-09-30 14:28: grad_clip: 0.0
2025-09-30 14:28: grad_norm: True
2025-09-30 14:28: graph_tag: grid8
2025-09-30 14:28: hidden_dim: 64
2025-09-30 14:28: holiday_list: []
2025-09-30 14:28: horizon: 12
2025-09-30 14:28: init_from: 
2025-09-30 14:28: input_base_dim: 1
2025-09-30 14:28: input_extra_dim: 2
2025-09-30 14:28: interval: 120
2025-09-30 14:28: json_name: eval_results.json
2025-09-30 14:28: kappa_nd: 0.05
2025-09-30 14:28: lag: 12
2025-09-30 14:28: lambda_phys: 0.2
2025-09-30 14:28: load_pretrain_path: /GPTST_ada.pth
2025-09-30 14:28: log_dir: outputs/GIMtec/gwn/seed_12/20250930_142816
2025-09-30 14:28: log_step: 20
2025-09-30 14:28: loss_func: mask_mae
2025-09-30 14:28: lr_decay: True
2025-09-30 14:28: lr_decay_rate: 0.3
2025-09-30 14:28: lr_decay_step: 25, 50, 75
2025-09-30 14:28: lr_init: 0.001
2025-09-30 14:28: mae_thresh: None
2025-09-30 14:28: mape_thresh: 0.001
2025-09-30 14:28: mask_ratio: 0.25
2025-09-30 14:28: max_grad_norm: 5
2025-09-30 14:28: min_lr: 0.0
2025-09-30 14:28: mode: ori
2025-09-30 14:28: model: GWN
2025-09-30 14:28: nochem: False
2025-09-30 14:28: normalizer: tec01
2025-09-30 14:28: num_nodes: 5183
2025-09-30 14:28: num_route: 2
2025-09-30 14:28: optimizer: adam
2025-09-30 14:28: output_dim: 1
2025-09-30 14:28: plateau_cooldown: 10
2025-09-30 14:28: plateau_factor: 0.9
2025-09-30 14:28: plateau_patience: 20
2025-09-30 14:28: plateau_threshold: 1e-05
2025-09-30 14:28: plateau_threshold_mode: abs
2025-09-30 14:28: plot: False
2025-09-30 14:28: prefix_boundary: True
2025-09-30 14:28: real_value: False
2025-09-30 14:28: rot_cap: 0.5
2025-09-30 14:28: roti_cap_scale: 0.7
2025-09-30 14:28: save_json: False
2025-09-30 14:28: save_pretrain_path: new_pretrain_model.pth
2025-09-30 14:28: scaler_zeros: 0.0
2025-09-30 14:28: scaler_zeros_day: 0
2025-09-30 14:28: scaler_zeros_week: 0
2025-09-30 14:28: scheduler: none
2025-09-30 14:28: seed: 12
2025-09-30 14:28: seed_mode: True
2025-09-30 14:28: stride_horizon: False
2025-09-30 14:28: t_ref_sec: 7200.0
2025-09-30 14:28: target_model: generic
2025-09-30 14:28: tec_ref: 50.0
2025-09-30 14:28: test_ratio: 0.1
2025-09-30 14:28: tod: False
2025-09-30 14:28: up_epoch: 6, 8
2025-09-30 14:28: use_adv: False
2025-09-30 14:28: use_diffusion: True
2025-09-30 14:28: use_drivers: True
2025-09-30 14:28: use_pinn: True
2025-09-30 14:28: val_ratio: 0.1
2025-09-30 14:28: week_day: 7
2025-09-30 14:28: week_start: 4
2025-09-30 14:28: weight_decay: 0.0
2025-09-30 14:28: xavier: False
2025-09-30 14:28: year_split: False
2025-09-30 14:28: PPINN parameters added to optimizer.
2025-09-30 14:28: Train Epoch 1: 0/6133 Loss: 18.356224 (running_avg: 18.356224)
2025-09-30 14:28: Train Epoch 1: 20/6133 Loss: 5.066581 (running_avg: 7.850973)
2025-09-30 14:29: Train Epoch 1: 40/6133 Loss: 6.756350 (running_avg: 6.610380)
2025-09-30 14:29: Train Epoch 1: 60/6133 Loss: 4.702108 (running_avg: 5.818571)
2025-09-30 14:29: Train Epoch 1: 80/6133 Loss: 3.315918 (running_avg: 5.451747)
2025-09-30 14:29: Train Epoch 1: 100/6133 Loss: 4.533032 (running_avg: 5.139479)
2025-09-30 14:29: Train Epoch 1: 120/6133 Loss: 3.808642 (running_avg: 4.893886)
2025-09-30 14:29: Train Epoch 1: 140/6133 Loss: 3.769604 (running_avg: 4.744382)
2025-09-30 14:29: Train Epoch 1: 160/6133 Loss: 2.392025 (running_avg: 4.585204)
2025-09-30 14:29: Train Epoch 1: 180/6133 Loss: 2.689434 (running_avg: 4.413240)
2025-09-30 14:29: Train Epoch 1: 200/6133 Loss: 3.976912 (running_avg: 4.280539)
2025-09-30 14:29: Train Epoch 1: 220/6133 Loss: 3.223442 (running_avg: 4.181432)
2025-09-30 14:29: Train Epoch 1: 240/6133 Loss: 2.566699 (running_avg: 4.109290)
2025-09-30 14:29: Train Epoch 1: 260/6133 Loss: 3.733985 (running_avg: 4.083400)
2025-09-30 14:29: Train Epoch 1: 280/6133 Loss: 2.825714 (running_avg: 4.005000)
2025-09-30 14:29: Train Epoch 1: 300/6133 Loss: 2.863003 (running_avg: 3.949726)
2025-09-30 14:29: Train Epoch 1: 320/6133 Loss: 3.275373 (running_avg: 3.909373)
2025-09-30 14:30: Train Epoch 1: 340/6133 Loss: 2.002214 (running_avg: 3.848927)
2025-09-30 14:30: Train Epoch 1: 360/6133 Loss: 2.095001 (running_avg: 3.793285)
2025-09-30 14:30: Train Epoch 1: 380/6133 Loss: 6.537831 (running_avg: 3.755423)
2025-09-30 14:30: Train Epoch 1: 400/6133 Loss: 2.261089 (running_avg: 3.690965)
2025-09-30 14:30: Train Epoch 1: 420/6133 Loss: 2.070795 (running_avg: 3.636504)
2025-09-30 14:30: Train Epoch 1: 440/6133 Loss: 2.753500 (running_avg: 3.612749)
2025-09-30 14:30: Train Epoch 1: 460/6133 Loss: 5.575278 (running_avg: 3.623573)
2025-09-30 14:30: Train Epoch 1: 480/6133 Loss: 8.116398 (running_avg: 3.594621)
2025-09-30 14:30: Train Epoch 1: 500/6133 Loss: 2.432379 (running_avg: 3.570900)
2025-09-30 14:30: Train Epoch 1: 520/6133 Loss: 2.385905 (running_avg: 3.546417)
2025-09-30 14:30: Train Epoch 1: 540/6133 Loss: 2.352483 (running_avg: 3.526001)
2025-09-30 14:30: Train Epoch 1: 560/6133 Loss: 3.237269 (running_avg: 3.504461)
2025-09-30 14:30: Train Epoch 1: 580/6133 Loss: 2.734650 (running_avg: 3.487552)
2025-09-30 14:30: Train Epoch 1: 600/6133 Loss: 2.586072 (running_avg: 3.470320)
2025-09-30 14:30: Train Epoch 1: 620/6133 Loss: 2.465977 (running_avg: 3.459067)
2025-09-30 14:30: Train Epoch 1: 640/6133 Loss: 2.384167 (running_avg: 3.435242)
2025-09-30 14:31: Train Epoch 1: 660/6133 Loss: 3.161842 (running_avg: 3.416507)
2025-09-30 14:31: Train Epoch 1: 680/6133 Loss: 2.227508 (running_avg: 3.389724)
2025-09-30 14:31: Train Epoch 1: 700/6133 Loss: 2.502067 (running_avg: 3.370935)
2025-09-30 14:31: Train Epoch 1: 720/6133 Loss: 2.387949 (running_avg: 3.348452)
2025-09-30 14:31: Train Epoch 1: 740/6133 Loss: 2.302953 (running_avg: 3.327419)
2025-09-30 14:31: Train Epoch 1: 760/6133 Loss: 2.397761 (running_avg: 3.301721)
2025-09-30 14:31: Train Epoch 1: 780/6133 Loss: 3.774103 (running_avg: 3.289038)
2025-09-30 14:31: Train Epoch 1: 800/6133 Loss: 3.142802 (running_avg: 3.268565)
2025-09-30 14:31: Train Epoch 1: 820/6133 Loss: 2.538726 (running_avg: 3.246367)
2025-09-30 14:31: Train Epoch 1: 840/6133 Loss: 2.328737 (running_avg: 3.245688)
2025-09-30 14:31: Train Epoch 1: 860/6133 Loss: 1.572593 (running_avg: 3.230119)
2025-09-30 14:31: Train Epoch 1: 880/6133 Loss: 2.608367 (running_avg: 3.225005)
2025-09-30 14:31: Train Epoch 1: 900/6133 Loss: 2.571602 (running_avg: 3.209962)
2025-09-30 14:31: Train Epoch 1: 920/6133 Loss: 2.010389 (running_avg: 3.201038)
2025-09-30 14:31: Train Epoch 1: 940/6133 Loss: 1.883936 (running_avg: 3.188998)
2025-09-30 14:31: Train Epoch 1: 960/6133 Loss: 3.304675 (running_avg: 3.179795)
2025-09-30 14:32: Train Epoch 1: 980/6133 Loss: 1.857888 (running_avg: 3.159742)
2025-09-30 14:32: Train Epoch 1: 1000/6133 Loss: 2.477715 (running_avg: 3.149579)
2025-09-30 14:32: Train Epoch 1: 1020/6133 Loss: 2.458941 (running_avg: 3.135075)
2025-09-30 14:32: Train Epoch 1: 1040/6133 Loss: 2.917890 (running_avg: 3.120609)
2025-09-30 14:32: Train Epoch 1: 1060/6133 Loss: 6.133715 (running_avg: 3.111132)
2025-09-30 14:32: Train Epoch 1: 1080/6133 Loss: 2.410348 (running_avg: 3.098269)
2025-09-30 14:32: Train Epoch 1: 1100/6133 Loss: 1.851268 (running_avg: 3.083351)
2025-09-30 14:32: Train Epoch 1: 1120/6133 Loss: 3.162100 (running_avg: 3.070118)
2025-09-30 14:32: Train Epoch 1: 1140/6133 Loss: 1.962970 (running_avg: 3.053953)
2025-09-30 14:32: Train Epoch 1: 1160/6133 Loss: 2.554268 (running_avg: 3.047020)
2025-09-30 14:32: Train Epoch 1: 1180/6133 Loss: 2.410203 (running_avg: 3.035865)
2025-09-30 14:32: Train Epoch 1: 1200/6133 Loss: 2.061628 (running_avg: 3.026845)
2025-09-30 14:32: Train Epoch 1: 1220/6133 Loss: 2.254010 (running_avg: 3.011487)
2025-09-30 14:32: Train Epoch 1: 1240/6133 Loss: 1.823664 (running_avg: 2.997837)
2025-09-30 14:32: Train Epoch 1: 1260/6133 Loss: 2.892682 (running_avg: 2.982338)
2025-09-30 14:32: Train Epoch 1: 1280/6133 Loss: 2.037816 (running_avg: 2.967488)
2025-09-30 14:33: Train Epoch 1: 1300/6133 Loss: 2.229881 (running_avg: 2.959284)
2025-09-30 14:33: Train Epoch 1: 1320/6133 Loss: 2.792351 (running_avg: 2.950222)
2025-09-30 14:33: Train Epoch 1: 1340/6133 Loss: 2.375232 (running_avg: 2.937974)
