2025-09-30 14:25: Experiment log path in: outputs/GIMtec/gwn/seed_12/20250930_142530
2025-09-30 14:25: Command: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False
2025-09-30 14:25: Torch: 2.7.0+cu128  CUDA: True
2025-09-30 14:25: GPU: NVIDIA H800 PCIe
2025-09-30 14:25: CUDA_VISIBLE_DEVICES=None
2025-09-30 14:25: HS: 10
2025-09-30 14:25: HT: 16
2025-09-30 14:25: HT_Tem: 8
2025-09-30 14:25: _run_start_ts: 20250930_142530
2025-09-30 14:25: _train_start_time: 1759213559.2458234
2025-09-30 14:25: accumulate_steps: 1
2025-09-30 14:25: ada_mask_ratio: 1.0
2025-09-30 14:25: ada_type: all
2025-09-30 14:25: amp: True
2025-09-30 14:25: batch_size: 192
2025-09-30 14:25: change_epoch: 5
2025-09-30 14:25: cmdline: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False
2025-09-30 14:25: column_wise: False
2025-09-30 14:25: config: 
2025-09-30 14:25: cosine_t_max: 100
2025-09-30 14:25: cuda: True
2025-09-30 14:25: dataset: GIMtec
2025-09-30 14:25: debug: False
2025-09-30 14:25: debug_max_steps: 1000
2025-09-30 14:25: default_graph: True
2025-09-30 14:25: device: cuda
2025-09-30 14:25: early_stop: True
2025-09-30 14:25: early_stop_min_delta: 0.0
2025-09-30 14:25: early_stop_patience: 20
2025-09-30 14:25: embed_dim: 16
2025-09-30 14:25: embed_dim_spa: 4
2025-09-30 14:25: epochs: 200
2025-09-30 14:25: eta_min_factor: 0.01
2025-09-30 14:25: grad_clip: 0.0
2025-09-30 14:25: grad_norm: True
2025-09-30 14:25: graph_tag: grid8
2025-09-30 14:25: hidden_dim: 64
2025-09-30 14:25: holiday_list: []
2025-09-30 14:25: horizon: 12
2025-09-30 14:25: init_from: 
2025-09-30 14:25: input_base_dim: 1
2025-09-30 14:25: input_extra_dim: 2
2025-09-30 14:25: interval: 120
2025-09-30 14:25: json_name: eval_results.json
2025-09-30 14:25: kappa_nd: 0.05
2025-09-30 14:25: lag: 12
2025-09-30 14:25: lambda_phys: 0.2
2025-09-30 14:25: load_pretrain_path: /GPTST_ada.pth
2025-09-30 14:25: log_dir: outputs/GIMtec/gwn/seed_12/20250930_142530
2025-09-30 14:25: log_step: 20
2025-09-30 14:25: loss_func: mask_mae
2025-09-30 14:25: lr_decay: True
2025-09-30 14:25: lr_decay_rate: 0.3
2025-09-30 14:25: lr_decay_step: 25, 50, 75
2025-09-30 14:25: lr_init: 0.001
2025-09-30 14:25: mae_thresh: None
2025-09-30 14:25: mape_thresh: 0.001
2025-09-30 14:25: mask_ratio: 0.25
2025-09-30 14:25: max_grad_norm: 5
2025-09-30 14:25: min_lr: 0.0
2025-09-30 14:25: mode: ori
2025-09-30 14:25: model: GWN
2025-09-30 14:25: nochem: False
2025-09-30 14:25: normalizer: tec01
2025-09-30 14:25: num_nodes: 5183
2025-09-30 14:25: num_route: 2
2025-09-30 14:25: optimizer: adam
2025-09-30 14:25: output_dim: 1
2025-09-30 14:25: plateau_cooldown: 10
2025-09-30 14:25: plateau_factor: 0.9
2025-09-30 14:25: plateau_patience: 20
2025-09-30 14:25: plateau_threshold: 1e-05
2025-09-30 14:25: plateau_threshold_mode: abs
2025-09-30 14:25: plot: False
2025-09-30 14:25: prefix_boundary: True
2025-09-30 14:25: real_value: False
2025-09-30 14:25: rot_cap: 0.5
2025-09-30 14:25: roti_cap_scale: 0.7
2025-09-30 14:25: save_json: False
2025-09-30 14:25: save_pretrain_path: new_pretrain_model.pth
2025-09-30 14:25: scaler_zeros: 0.0
2025-09-30 14:25: scaler_zeros_day: 0
2025-09-30 14:25: scaler_zeros_week: 0
2025-09-30 14:25: scheduler: none
2025-09-30 14:25: seed: 12
2025-09-30 14:25: seed_mode: True
2025-09-30 14:25: stride_horizon: False
2025-09-30 14:25: t_ref_sec: 7200.0
2025-09-30 14:25: target_model: generic
2025-09-30 14:25: tec_ref: 50.0
2025-09-30 14:25: test_ratio: 0.1
2025-09-30 14:25: tod: False
2025-09-30 14:25: up_epoch: 6, 8
2025-09-30 14:25: use_adv: False
2025-09-30 14:25: use_diffusion: True
2025-09-30 14:25: use_drivers: True
2025-09-30 14:25: use_pinn: True
2025-09-30 14:25: val_ratio: 0.1
2025-09-30 14:25: week_day: 7
2025-09-30 14:25: week_start: 4
2025-09-30 14:25: weight_decay: 0.0
2025-09-30 14:25: xavier: False
2025-09-30 14:25: year_split: False
2025-09-30 14:25: PPINN parameters added to optimizer.
2025-09-30 14:26: Train Epoch 1: 0/256 Loss: 18.314451 (running_avg: 18.314451)
