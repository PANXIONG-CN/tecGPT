[data]
num_nodes = 5183
input_window = 12
output_window = 12

[model]
# 注意：d_llm=768 对应 GPT-2 Base 维度；如无 transformers 将退化为 TinyEncoder
# 轻时空嵌入：默认开启（等维残差建议 heads*spatial_out == base_C + 2*d_e == 33 ）
use_node_tod_emb = True
d_e = 16
tod_bins = 12

# 示例：等维 33 → heads=3, spatial_out=11（如不满足等维，将在运行时关闭该处残差并告警）
heads = 3
spatial_out = 11
temporal_channels = (64,128)
temporal_strides = (2,2)
patch_len = 4
d_llm = 1280
llm_layers = 3
node_chunk = 512
graph_tag = grid8

# LoRA（默认关闭；需 peft 支持）
use_lora = True
lora_r = 32
lora_alpha = 64
lora_targets = c_attn
dropout_after_llm = 0.1
hf_model_name = /root/autodl-tmp/cache/hub/models--openai-community--gpt2-large/snapshots/32b71b12589c2f8d625668d2335a01cac3249519

[train]
amp = True
scheduler = cosine
accumulate_steps = 6
lr_init = 3e-4
plateau_factor = 0.5
plateau_patience = 2
plateau_threshold = 0.001
plateau_threshold_mode = rel
plateau_cooldown = 0
min_lr = 1e-5
