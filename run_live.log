/root/tecGPT
dataset : GIMtec
mode : ori
device : cuda
model : GWN
cuda : True
val_ratio : 0.1
test_ratio : 0.1
lag : 12
horizon : 12
num_nodes : 5183
tod : False
normalizer : tec01
column_wise : False
default_graph : True
input_base_dim : 1
input_extra_dim : 2
output_dim : 1
embed_dim : 16
embed_dim_spa : 4
hidden_dim : 64
HS : 10
HT : 16
HT_Tem : 8
num_route : 2
mask_ratio : 0.25
ada_mask_ratio : 1.0
ada_type : all
loss_func : mask_mae
seed : 12
batch_size : 192
epochs : 1
lr_init : 0.003
lr_decay : True
lr_decay_rate : 0.3
lr_decay_step : 25, 50, 75
early_stop : True
early_stop_patience : 25
change_epoch : 5
up_epoch : 6, 8
grad_norm : True
max_grad_norm : 5
debug : False
debug_max_steps : 1000
real_value : False
optimizer : adam
weight_decay : 0.0
scheduler : none
plateau_factor : 0.9
plateau_patience : 20
plateau_threshold : 1e-05
plateau_threshold_mode : abs
plateau_cooldown : 10
min_lr : 0.0
cosine_t_max : 100
eta_min_factor : 0.01
grad_clip : 0.0
early_stop_min_delta : 0.0
accumulate_steps : 2
amp : True
seed_mode : True
xavier : False
load_pretrain_path : /GPTST_ada.pth
save_pretrain_path : new_pretrain_model.pth
init_from : 
stride_horizon : False
prefix_boundary : True
year_split : True
target_model : generic
graph_tag : grid8
use_pinn : True
lambda_phys : 0.2
use_diffusion : True
use_drivers : True
use_adv : True
rot_cap : 0.5
roti_cap_scale : 0.7
kappa_nd : 0.05
tec_ref : 50.0
t_ref_sec : 7200.0
mae_thresh : None
mape_thresh : 0.001
log_dir : ./
log_step : 10
plot : False
save_json : False
json_name : eval_results.json
==========
batch_size : 64
epochs : 100
lr_init : 0.003
lr_decay : True
lr_decay_rate : 0.3
lr_decay_step : 25, 50, 75
early_stop : True
early_stop_patience : 25
grad_norm : True
max_grad_norm : 5
debug : False
real_value : False
num_nodes : 5183
input_window : 12
output_window : 12
output_dim : 1
dropout : 0.3
blocks : 4
layers : 2
gcn_bool : True
addaptadj : True
adjtype : doubletransition
randomadj : True
aptonly : False
kernel_size : 2
nhid : 32
residual_channels : 32
dilation_channels : 32
graph_tag : grid8
adj_model : 
seed : 12
seed_mode : True
xavier : False
loss_func : mask_mae
filepath : ../data/GIMtec/
filename : GIMtec
adj_mx : [[1. 1. 0. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]
 [0. 1. 1. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 1. 1. 0.]
 [0. 0. 0. ... 1. 1. 1.]
 [0. 0. 0. ... 0. 1. 1.]]
mode:  ori   model:  GWN   dataset:  GIMtec   load_pretrain_path:  /GPTST_ada.pth   save_pretrain_path:  new_pretrain_model.pth
*****************Model Parameter*****************
predictor.nodevec1 torch.Size([5183, 10]) True
predictor.nodevec2 torch.Size([10, 5183]) True
predictor.filter_convs.0.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.0.bias torch.Size([32]) True
predictor.filter_convs.1.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.1.bias torch.Size([32]) True
predictor.filter_convs.2.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.2.bias torch.Size([32]) True
predictor.filter_convs.3.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.3.bias torch.Size([32]) True
predictor.filter_convs.4.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.4.bias torch.Size([32]) True
predictor.filter_convs.5.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.5.bias torch.Size([32]) True
predictor.filter_convs.6.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.6.bias torch.Size([32]) True
predictor.filter_convs.7.weight torch.Size([32, 32, 1, 2]) True
predictor.filter_convs.7.bias torch.Size([32]) True
predictor.gate_convs.0.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.0.bias torch.Size([32]) True
predictor.gate_convs.1.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.1.bias torch.Size([32]) True
predictor.gate_convs.2.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.2.bias torch.Size([32]) True
predictor.gate_convs.3.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.3.bias torch.Size([32]) True
predictor.gate_convs.4.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.4.bias torch.Size([32]) True
predictor.gate_convs.5.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.5.bias torch.Size([32]) True
predictor.gate_convs.6.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.6.bias torch.Size([32]) True
predictor.gate_convs.7.weight torch.Size([32, 32, 1, 2]) True
predictor.gate_convs.7.bias torch.Size([32]) True
predictor.residual_convs.0.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.0.bias torch.Size([32]) True
predictor.residual_convs.1.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.1.bias torch.Size([32]) True
predictor.residual_convs.2.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.2.bias torch.Size([32]) True
predictor.residual_convs.3.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.3.bias torch.Size([32]) True
predictor.residual_convs.4.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.4.bias torch.Size([32]) True
predictor.residual_convs.5.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.5.bias torch.Size([32]) True
predictor.residual_convs.6.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.6.bias torch.Size([32]) True
predictor.residual_convs.7.weight torch.Size([32, 32, 1, 1]) True
predictor.residual_convs.7.bias torch.Size([32]) True
predictor.skip_convs.0.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.0.bias torch.Size([256]) True
predictor.skip_convs.1.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.1.bias torch.Size([256]) True
predictor.skip_convs.2.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.2.bias torch.Size([256]) True
predictor.skip_convs.3.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.3.bias torch.Size([256]) True
predictor.skip_convs.4.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.4.bias torch.Size([256]) True
predictor.skip_convs.5.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.5.bias torch.Size([256]) True
predictor.skip_convs.6.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.6.bias torch.Size([256]) True
predictor.skip_convs.7.weight torch.Size([256, 32, 1, 1]) True
predictor.skip_convs.7.bias torch.Size([256]) True
predictor.bn.0.weight torch.Size([32]) True
predictor.bn.0.bias torch.Size([32]) True
predictor.bn.1.weight torch.Size([32]) True
predictor.bn.1.bias torch.Size([32]) True
predictor.bn.2.weight torch.Size([32]) True
predictor.bn.2.bias torch.Size([32]) True
predictor.bn.3.weight torch.Size([32]) True
predictor.bn.3.bias torch.Size([32]) True
predictor.bn.4.weight torch.Size([32]) True
predictor.bn.4.bias torch.Size([32]) True
predictor.bn.5.weight torch.Size([32]) True
predictor.bn.5.bias torch.Size([32]) True
predictor.bn.6.weight torch.Size([32]) True
predictor.bn.6.bias torch.Size([32]) True
predictor.bn.7.weight torch.Size([32]) True
predictor.bn.7.bias torch.Size([32]) True
predictor.gconv.0.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.0.mlp.mlp.bias torch.Size([32]) True
predictor.gconv.1.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.1.mlp.mlp.bias torch.Size([32]) True
predictor.gconv.2.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.2.mlp.mlp.bias torch.Size([32]) True
predictor.gconv.3.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.3.mlp.mlp.bias torch.Size([32]) True
predictor.gconv.4.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.4.mlp.mlp.bias torch.Size([32]) True
predictor.gconv.5.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.5.mlp.mlp.bias torch.Size([32]) True
predictor.gconv.6.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.6.mlp.mlp.bias torch.Size([32]) True
predictor.gconv.7.mlp.mlp.weight torch.Size([32, 224, 1, 1]) True
predictor.gconv.7.mlp.mlp.bias torch.Size([32]) True
predictor.start_conv.weight torch.Size([32, 1, 1, 1]) True
predictor.start_conv.bias torch.Size([32]) True
predictor.end_conv_1.weight torch.Size([512, 256, 1, 1]) True
predictor.end_conv_1.bias torch.Size([512]) True
predictor.end_conv_2.weight torch.Size([12, 512, 1, 1]) True
predictor.end_conv_2.bias torch.Size([12]) True
Total params num: 408888, Update params num: 408888
*****************Finish Parameter****************
============================scaler_mae_loss
Applying learning rate decay.
Creat Log File in:  /root/tecGPT/Output/GIMtec/GWN/20250925_123258_GIMtec_GWN_ori.log
2025-09-25 12:32: Experiment log path in: /root/tecGPT/Output/GIMtec/GWN
2025-09-25 12:32: Command: python Run.py -dataset GIMtec -model GWN -mode ori -use_pinn True -lambda_phys 0.2 -use_drivers True -use_adv True -year_split True -epochs 1 -batch_size 192 -accumulate_steps 2 -scheduler none -amp True -log_step 10
2025-09-25 12:32: Torch: 2.7.0+cu128  CUDA: True
2025-09-25 12:32: GPU: NVIDIA H800 PCIe
2025-09-25 12:32: CUDA_VISIBLE_DEVICES=None
2025-09-25 12:32: HS: 10
2025-09-25 12:32: HT: 16
2025-09-25 12:32: HT_Tem: 8
2025-09-25 12:32: accumulate_steps: 2
2025-09-25 12:32: ada_mask_ratio: 1.0
2025-09-25 12:32: ada_type: all
2025-09-25 12:32: amp: True
2025-09-25 12:32: batch_size: 192
2025-09-25 12:32: change_epoch: 5
2025-09-25 12:32: cmdline: python Run.py -dataset GIMtec -model GWN -mode ori -use_pinn True -lambda_phys 0.2 -use_drivers True -use_adv True -year_split True -epochs 1 -batch_size 192 -accumulate_steps 2 -scheduler none -amp True -log_step 10
2025-09-25 12:32: column_wise: False
2025-09-25 12:32: cosine_t_max: 100
2025-09-25 12:32: cuda: True
2025-09-25 12:32: dataset: GIMtec
2025-09-25 12:32: debug: False
2025-09-25 12:32: debug_max_steps: 1000
2025-09-25 12:32: default_graph: True
2025-09-25 12:32: device: cuda
2025-09-25 12:32: early_stop: True
2025-09-25 12:32: early_stop_min_delta: 0.0
2025-09-25 12:32: early_stop_patience: 25
2025-09-25 12:32: embed_dim: 16
2025-09-25 12:32: embed_dim_spa: 4
2025-09-25 12:32: epochs: 1
2025-09-25 12:32: eta_min_factor: 0.01
2025-09-25 12:32: grad_clip: 0.0
2025-09-25 12:32: grad_norm: True
2025-09-25 12:32: graph_tag: grid8
2025-09-25 12:32: hidden_dim: 64
2025-09-25 12:32: horizon: 12
2025-09-25 12:32: init_from: 
2025-09-25 12:32: input_base_dim: 1
2025-09-25 12:32: input_extra_dim: 2
2025-09-25 12:32: interval: 120
2025-09-25 12:32: json_name: eval_results.json
2025-09-25 12:32: kappa_nd: 0.05
2025-09-25 12:32: lag: 12
2025-09-25 12:32: lambda_phys: 0.2
2025-09-25 12:32: load_pretrain_path: /GPTST_ada.pth
2025-09-25 12:32: log_dir: /root/tecGPT/Output/GIMtec/GWN
2025-09-25 12:32: log_step: 10
2025-09-25 12:32: loss_func: mask_mae
2025-09-25 12:32: lr_decay: True
2025-09-25 12:32: lr_decay_rate: 0.3
2025-09-25 12:32: lr_decay_step: 25, 50, 75
2025-09-25 12:32: lr_init: 0.003
2025-09-25 12:32: mae_thresh: None
2025-09-25 12:32: mape_thresh: 0.001
2025-09-25 12:32: mask_ratio: 0.25
2025-09-25 12:32: max_grad_norm: 5
2025-09-25 12:32: min_lr: 0.0
2025-09-25 12:32: mode: ori
2025-09-25 12:32: model: GWN
2025-09-25 12:32: normalizer: tec01
2025-09-25 12:32: num_nodes: 5183
2025-09-25 12:32: num_route: 2
2025-09-25 12:32: optimizer: adam
2025-09-25 12:32: output_dim: 1
2025-09-25 12:32: plateau_cooldown: 10
2025-09-25 12:32: plateau_factor: 0.9
2025-09-25 12:32: plateau_patience: 20
2025-09-25 12:32: plateau_threshold: 1e-05
2025-09-25 12:32: plateau_threshold_mode: abs
2025-09-25 12:32: plot: False
2025-09-25 12:32: prefix_boundary: True
2025-09-25 12:32: real_value: False
2025-09-25 12:32: rot_cap: 0.5
2025-09-25 12:32: roti_cap_scale: 0.7
2025-09-25 12:32: save_json: False
2025-09-25 12:32: save_pretrain_path: new_pretrain_model.pth
2025-09-25 12:32: scaler_zeros: 0.0
2025-09-25 12:32: scaler_zeros_day: 0
2025-09-25 12:32: scaler_zeros_week: 0
2025-09-25 12:32: scheduler: none
2025-09-25 12:32: seed: 12
2025-09-25 12:32: seed_mode: True
2025-09-25 12:32: stride_horizon: False
2025-09-25 12:32: t_ref_sec: 7200.0
2025-09-25 12:32: target_model: generic
2025-09-25 12:32: tec_ref: 50.0
2025-09-25 12:32: test_ratio: 0.1
2025-09-25 12:32: tod: False
2025-09-25 12:32: up_epoch: 6, 8
2025-09-25 12:32: use_adv: True
2025-09-25 12:32: use_diffusion: True
2025-09-25 12:32: use_drivers: True
2025-09-25 12:32: use_pinn: True
2025-09-25 12:32: val_ratio: 0.1
2025-09-25 12:32: week_day: 7
2025-09-25 12:32: weight_decay: 0.0
2025-09-25 12:32: xavier: False
2025-09-25 12:32: year_split: True
2025-09-25 12:32: PPINN parameters added to optimizer.
2025-09-25 12:33: Train Epoch 1: 0/160 Loss: 16.690046 (running_avg: 16.690046)
2025-09-25 12:33: Train Epoch 1: 10/160 Loss: 9.143794 (running_avg: 16.116426)
2025-09-25 12:34: Train Epoch 1: 20/160 Loss: 6.454872 (running_avg: 11.942142)
2025-09-25 12:35: Train Epoch 1: 30/160 Loss: 4.470204 (running_avg: 9.815545)
2025-09-25 12:35: Train Epoch 1: 40/160 Loss: 3.644577 (running_avg: 8.408439)
2025-09-25 12:36: Train Epoch 1: 50/160 Loss: 3.472581 (running_avg: 7.424216)
2025-09-25 12:36: Train Epoch 1: 60/160 Loss: 2.927022 (running_avg: 6.735927)
2025-09-25 12:37: Train Epoch 1: 70/160 Loss: 3.028624 (running_avg: 6.218174)
2025-09-25 12:38: Train Epoch 1: 80/160 Loss: 3.038716 (running_avg: 5.814229)
2025-09-25 12:38: Train Epoch 1: 90/160 Loss: 2.572847 (running_avg: 5.466424)
2025-09-25 12:39: Train Epoch 1: 100/160 Loss: 2.518393 (running_avg: 5.174735)
2025-09-25 12:40: Train Epoch 1: 110/160 Loss: 2.771824 (running_avg: 4.926636)
2025-09-25 12:40: Train Epoch 1: 120/160 Loss: 2.388907 (running_avg: 4.714698)
2025-09-25 12:41: Train Epoch 1: 130/160 Loss: 2.434212 (running_avg: 4.532400)
2025-09-25 12:42: Train Epoch 1: 140/160 Loss: 2.602792 (running_avg: 4.389289)
2025-09-25 12:42: Train Epoch 1: 150/160 Loss: 2.366625 (running_avg: 4.261401)
2025-09-25 12:43: **********Train Epoch 1: averaged Loss: 4.177047 (per-batch mean)
2025-09-25 12:44: **********Val Epoch 1: average Loss: 2.356255
2025-09-25 12:44: *********************************Current best model saved!
2025-09-25 12:44: Total training time: 11.4789min, best loss: 2.356255
2025-09-25 12:44: Saving current model to /root/tecGPT/Output/GIMtec/GWN/20250925_123258_GIMtec_GWN_ori.pth
2025-09-25 12:48: [test] Horizon 01, MAE: 1.85, RMSE: 2.99, CORR:0.9671 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 02, MAE: 2.39, RMSE: 3.46, CORR:0.9492 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 03, MAE: 2.38, RMSE: 3.67, CORR:0.9424 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 04, MAE: 2.41, RMSE: 3.49, CORR:0.9470 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 05, MAE: 2.34, RMSE: 3.52, CORR:0.9465 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 06, MAE: 2.21, RMSE: 3.57, CORR:0.9467 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 07, MAE: 1.99, RMSE: 3.20, CORR:0.9509 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 08, MAE: 2.00, RMSE: 3.21, CORR:0.9506 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 09, MAE: 2.02, RMSE: 3.20, CORR:0.9509 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 10, MAE: 1.98, RMSE: 3.11, CORR:0.9545 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 11, MAE: 2.03, RMSE: 3.30, CORR:0.9521 (valid=100.0%)
2025-09-25 12:48: [test] Horizon 12, MAE: 2.04, RMSE: 3.43, CORR:0.9517 (valid=100.0%)
2025-09-25 12:48: [test] Average Horizon, MAE: 2.14, RMSE: 3.35, CORR:0.9508
2025-09-25 12:48: Failed to save arrays: Not enough free space to write 2718835944 bytes
2025-09-25 12:49: [val] Horizon 01, MAE: 2.17, RMSE: 3.37, CORR:0.9659 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 02, MAE: 2.63, RMSE: 3.79, CORR:0.9477 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 03, MAE: 2.75, RMSE: 4.10, CORR:0.9405 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 04, MAE: 2.65, RMSE: 3.81, CORR:0.9451 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 05, MAE: 2.63, RMSE: 3.88, CORR:0.9445 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 06, MAE: 2.57, RMSE: 3.98, CORR:0.9452 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 07, MAE: 2.31, RMSE: 3.56, CORR:0.9493 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 08, MAE: 2.29, RMSE: 3.56, CORR:0.9496 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 09, MAE: 2.32, RMSE: 3.55, CORR:0.9496 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 10, MAE: 2.24, RMSE: 3.42, CORR:0.9535 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 11, MAE: 2.33, RMSE: 3.66, CORR:0.9506 (valid=100.0%)
2025-09-25 12:49: [val] Horizon 12, MAE: 2.42, RMSE: 3.84, CORR:0.9504 (valid=100.0%)
2025-09-25 12:49: [val] Average Horizon, MAE: 2.44, RMSE: 3.72, CORR:0.9493
2025-09-25 12:49: Year-wise evaluation skipped due to error: ../data/GIMtec/TEC_2015.npy
