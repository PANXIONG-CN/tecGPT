2025-09-30 14:23: Experiment log path in: outputs/GIMtec/gwn/seed_12/20250930_142320
2025-09-30 14:23: Command: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False
2025-09-30 14:23: Torch: 2.7.0+cu128  CUDA: True
2025-09-30 14:23: GPU: NVIDIA H800 PCIe
2025-09-30 14:23: CUDA_VISIBLE_DEVICES=None
2025-09-30 14:23: HS: 10
2025-09-30 14:23: HT: 16
2025-09-30 14:23: HT_Tem: 8
2025-09-30 14:23: _run_start_ts: 20250930_142320
2025-09-30 14:23: _train_start_time: 1759213428.8271477
2025-09-30 14:23: accumulate_steps: 1
2025-09-30 14:23: ada_mask_ratio: 1.0
2025-09-30 14:23: ada_type: all
2025-09-30 14:23: amp: True
2025-09-30 14:23: batch_size: 192
2025-09-30 14:23: change_epoch: 5
2025-09-30 14:23: cmdline: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False
2025-09-30 14:23: column_wise: False
2025-09-30 14:23: config: 
2025-09-30 14:23: cosine_t_max: 100
2025-09-30 14:23: cuda: True
2025-09-30 14:23: dataset: GIMtec
2025-09-30 14:23: debug: False
2025-09-30 14:23: debug_max_steps: 1000
2025-09-30 14:23: default_graph: True
2025-09-30 14:23: device: cuda
2025-09-30 14:23: early_stop: True
2025-09-30 14:23: early_stop_min_delta: 0.0
2025-09-30 14:23: early_stop_patience: 20
2025-09-30 14:23: embed_dim: 16
2025-09-30 14:23: embed_dim_spa: 4
2025-09-30 14:23: epochs: 200
2025-09-30 14:23: eta_min_factor: 0.01
2025-09-30 14:23: grad_clip: 0.0
2025-09-30 14:23: grad_norm: True
2025-09-30 14:23: graph_tag: grid8
2025-09-30 14:23: hidden_dim: 64
2025-09-30 14:23: holiday_list: []
2025-09-30 14:23: horizon: 12
2025-09-30 14:23: init_from: 
2025-09-30 14:23: input_base_dim: 1
2025-09-30 14:23: input_extra_dim: 2
2025-09-30 14:23: interval: 120
2025-09-30 14:23: json_name: eval_results.json
2025-09-30 14:23: kappa_nd: 0.05
2025-09-30 14:23: lag: 12
2025-09-30 14:23: lambda_phys: 0.2
2025-09-30 14:23: load_pretrain_path: /GPTST_ada.pth
2025-09-30 14:23: log_dir: outputs/GIMtec/gwn/seed_12/20250930_142320
2025-09-30 14:23: log_step: 20
2025-09-30 14:23: loss_func: mask_mae
2025-09-30 14:23: lr_decay: True
2025-09-30 14:23: lr_decay_rate: 0.3
2025-09-30 14:23: lr_decay_step: 25, 50, 75
2025-09-30 14:23: lr_init: 0.001
2025-09-30 14:23: mae_thresh: None
2025-09-30 14:23: mape_thresh: 0.001
2025-09-30 14:23: mask_ratio: 0.25
2025-09-30 14:23: max_grad_norm: 5
2025-09-30 14:23: min_lr: 0.0
2025-09-30 14:23: mode: ori
2025-09-30 14:23: model: GWN
2025-09-30 14:23: nochem: False
2025-09-30 14:23: normalizer: tec01
2025-09-30 14:23: num_nodes: 5183
2025-09-30 14:23: num_route: 2
2025-09-30 14:23: optimizer: adam
2025-09-30 14:23: output_dim: 1
2025-09-30 14:23: plateau_cooldown: 10
2025-09-30 14:23: plateau_factor: 0.9
2025-09-30 14:23: plateau_patience: 20
2025-09-30 14:23: plateau_threshold: 1e-05
2025-09-30 14:23: plateau_threshold_mode: abs
2025-09-30 14:23: plot: False
2025-09-30 14:23: prefix_boundary: True
2025-09-30 14:23: real_value: False
2025-09-30 14:23: rot_cap: 0.5
2025-09-30 14:23: roti_cap_scale: 0.7
2025-09-30 14:23: save_json: False
2025-09-30 14:23: save_pretrain_path: new_pretrain_model.pth
2025-09-30 14:23: scaler_zeros: 0.0
2025-09-30 14:23: scaler_zeros_day: 0
2025-09-30 14:23: scaler_zeros_week: 0
2025-09-30 14:23: scheduler: none
2025-09-30 14:23: seed: 12
2025-09-30 14:23: seed_mode: True
2025-09-30 14:23: stride_horizon: False
2025-09-30 14:23: t_ref_sec: 7200.0
2025-09-30 14:23: target_model: generic
2025-09-30 14:23: tec_ref: 50.0
2025-09-30 14:23: test_ratio: 0.1
2025-09-30 14:23: tod: False
2025-09-30 14:23: up_epoch: 6, 8
2025-09-30 14:23: use_adv: False
2025-09-30 14:23: use_diffusion: True
2025-09-30 14:23: use_drivers: True
2025-09-30 14:23: use_pinn: True
2025-09-30 14:23: val_ratio: 0.1
2025-09-30 14:23: week_day: 7
2025-09-30 14:23: week_start: 4
2025-09-30 14:23: weight_decay: 0.0
2025-09-30 14:23: xavier: False
2025-09-30 14:23: year_split: False
2025-09-30 14:23: PPINN parameters added to optimizer.
