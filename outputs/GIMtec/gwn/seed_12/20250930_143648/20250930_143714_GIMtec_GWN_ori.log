2025-09-30 14:37: Experiment log path in: outputs/GIMtec/gwn/seed_12/20250930_143648
2025-09-30 14:37: Command: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False -use_drivers False -year_split True -stride_horizon True
2025-09-30 14:37: Torch: 2.7.0+cu128  CUDA: False
2025-09-30 14:37: HS: 10
2025-09-30 14:37: HT: 16
2025-09-30 14:37: HT_Tem: 8
2025-09-30 14:37: _run_start_ts: 20250930_143648
2025-09-30 14:37: _train_start_time: 1759214234.5122082
2025-09-30 14:37: accumulate_steps: 1
2025-09-30 14:37: ada_mask_ratio: 1.0
2025-09-30 14:37: ada_type: all
2025-09-30 14:37: amp: True
2025-09-30 14:37: batch_size: 8
2025-09-30 14:37: change_epoch: 5
2025-09-30 14:37: cmdline: python Run.py -dataset GIMtec -mode ori -model GWN -epochs 1 -batch_size 8 -amp False -use_drivers False -year_split True -stride_horizon True
2025-09-30 14:37: column_wise: False
2025-09-30 14:37: config: 
2025-09-30 14:37: cosine_t_max: 100
2025-09-30 14:37: cuda: True
2025-09-30 14:37: dataset: GIMtec
2025-09-30 14:37: debug: False
2025-09-30 14:37: debug_max_steps: 1000
2025-09-30 14:37: default_graph: True
2025-09-30 14:37: device: cpu
2025-09-30 14:37: early_stop: True
2025-09-30 14:37: early_stop_min_delta: 0.0
2025-09-30 14:37: early_stop_patience: 20
2025-09-30 14:37: embed_dim: 16
2025-09-30 14:37: embed_dim_spa: 4
2025-09-30 14:37: epochs: 1
2025-09-30 14:37: eta_min_factor: 0.01
2025-09-30 14:37: grad_clip: 0.0
2025-09-30 14:37: grad_norm: True
2025-09-30 14:37: graph_tag: grid8
2025-09-30 14:37: hidden_dim: 64
2025-09-30 14:37: horizon: 12
2025-09-30 14:37: init_from: 
2025-09-30 14:37: input_base_dim: 1
2025-09-30 14:37: input_extra_dim: 2
2025-09-30 14:37: interval: 120
2025-09-30 14:37: json_name: eval_results.json
2025-09-30 14:37: kappa_nd: 0.05
2025-09-30 14:37: lag: 12
2025-09-30 14:37: lambda_phys: 0.2
2025-09-30 14:37: load_pretrain_path: /GPTST_ada.pth
2025-09-30 14:37: log_dir: outputs/GIMtec/gwn/seed_12/20250930_143648
2025-09-30 14:37: log_step: 20
2025-09-30 14:37: loss_func: mask_mae
2025-09-30 14:37: lr_decay: True
2025-09-30 14:37: lr_decay_rate: 0.3
2025-09-30 14:37: lr_decay_step: 25, 50, 75
2025-09-30 14:37: lr_init: 0.001
2025-09-30 14:37: mae_thresh: None
2025-09-30 14:37: mape_thresh: 0.001
2025-09-30 14:37: mask_ratio: 0.25
2025-09-30 14:37: max_grad_norm: 5
2025-09-30 14:37: min_lr: 0.0
2025-09-30 14:37: mode: ori
2025-09-30 14:37: model: GWN
2025-09-30 14:37: nochem: False
2025-09-30 14:37: normalizer: tec01
2025-09-30 14:37: num_nodes: 5183
2025-09-30 14:37: num_route: 2
2025-09-30 14:37: optimizer: adam
2025-09-30 14:37: output_dim: 1
2025-09-30 14:37: plateau_cooldown: 10
2025-09-30 14:37: plateau_factor: 0.9
2025-09-30 14:37: plateau_patience: 20
2025-09-30 14:37: plateau_threshold: 1e-05
2025-09-30 14:37: plateau_threshold_mode: abs
2025-09-30 14:37: plot: False
2025-09-30 14:37: prefix_boundary: True
2025-09-30 14:37: real_value: False
2025-09-30 14:37: rot_cap: 0.5
2025-09-30 14:37: roti_cap_scale: 0.7
2025-09-30 14:37: save_json: False
2025-09-30 14:37: save_pretrain_path: new_pretrain_model.pth
2025-09-30 14:37: scaler_zeros: 0.0
2025-09-30 14:37: scaler_zeros_day: 0
2025-09-30 14:37: scaler_zeros_week: 0
2025-09-30 14:37: scheduler: none
2025-09-30 14:37: seed: 12
2025-09-30 14:37: seed_mode: True
2025-09-30 14:37: stride_horizon: True
2025-09-30 14:37: t_ref_sec: 7200.0
2025-09-30 14:37: target_model: generic
2025-09-30 14:37: tec_ref: 50.0
2025-09-30 14:37: test_ratio: 0.1
2025-09-30 14:37: tod: False
2025-09-30 14:37: up_epoch: 6, 8
2025-09-30 14:37: use_adv: False
2025-09-30 14:37: use_diffusion: True
2025-09-30 14:37: use_drivers: True
2025-09-30 14:37: use_pinn: True
2025-09-30 14:37: val_ratio: 0.1
2025-09-30 14:37: week_day: 7
2025-09-30 14:37: weight_decay: 0.0
2025-09-30 14:37: xavier: False
2025-09-30 14:37: year_split: True
2025-09-30 14:37: PPINN parameters added to optimizer.
2025-09-30 14:37: Train Epoch 1: 0/320 Loss: 14.762306 (running_avg: 14.762306)
2025-09-30 14:41: Train Epoch 1: 20/320 Loss: 4.456936 (running_avg: 7.547858)
